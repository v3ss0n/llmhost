version: '3.8'
services:
  litellm:
    restart: unless-stopped
    # image: ghcr.io/berriai/litellm:v1.59.8
    image: litellm/litellm:latest
    volumes:
      - ./config.yaml:/app/config.yaml
    command:
      - "--config=/app/config.yaml"
    ports:
      - "0.0.0.0:4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
        LITELLM_MASTER_KEY: "sk-0xc0d3admin" # Your master key for the proxy server. Can use this to send /chat/completion requests etc
        LITELLM_SALT_KEY: "sk-XXXXXXXX" # Can NOT CHANGE THIS ONCE SET - It is used to encrypt/decrypt credentials stored in DB. If value of 'LITELLM_SALT_KEY' changes your models cannot be retrieved from DB
        DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
        STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
    env_file:
      - .env # Load local .env file
    networks:
      - llmhost
 
  db:
    image: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10
    volumes:
      - pg_data:/var/lib/postgresql
    networks:
      - llmhost
  # inference:
  #   image: ghcr.io/huggingface/text-generation-inference:latest
  #   ports:
  #     - "0.0.0.0:8099:80"
  #   restart: "unless-stopped"
  #   # command: "--model-id Qwen/Qwen2.5-Coder-32B-Instruct-AWQ --sharded true --num-shard 2 --kv-cache-dtype fp8_e4m3fn"
  #   command: "--model-id Qwen/QwQ-32B-AWQ --sharded true --num-shard 2 --kv-cache-dtype fp8_e4m3fn"
  #   # command: "--model-id mistralai/Mistral-Small-24B-Instruct-2501 --sharded true --num-shard 2 --quantize bitsandbytes-nf4 "
  #   # command: "--model-id mistralai/Mistral-Small-3.1-24B-Instruct-2503 --sharded true --num-shard 2"
  #   # command: "--model-id OPEA/Mistral-Small-3.1-24B-Instruct-2503-int4-AutoRound-awq-sym --sharded true --num-shard 2"
  #   # command: "--model-id RekaAI/reka-flash-3 --sharded true --num-shard 2 --quantize bitsandbytes-nf4"
  #   # command: "--model-id ISTA-DASLab/Mistral-Small-3.1-24B-Instruct-2503-GPTQ-4b-128g --sharded true --num-shard 2"
  #   # command: "--model-id mistralai/Mistral-Small-3.1-24B-Instruct-2503 --sharded true --num-shard 2"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0', '1']
  #             capabilities: [gpu]
  #   shm_size: '90g'
  #   volumes:
  #     - ~/.hf-docker-data:/data
  #   networks:
  #     - llmhost
  inference:
    image: vllm/vllm-openai:latest

    # command: --model Qwen/Qwen3-30B-A3B-fp8  --quantization fp8 --port 80  --enable-prefix-caching --reasoning-parser qwen3  --enable-auto-tool-choice --tool-call-parser hermes --enable-expert-parallel --enforce-eager
    # command: --model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2 --kv-cache-dtype fp8_e5m2
    command: --model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --enable-prefix-caching --kv-cache-dtype fp8_e4m3 --reasoning-parser qwen3  --enable-auto-tool-choice --tool-call-parser hermes 
    # command: --model Qwen/Qwen3-14B --port 80  --tensor-parallel-size 2 --quantization="fp8" --kv-cache-dtype fp8_e4m3  --enable-prefix-caching --reasoning-parser qwen3  --enable-auto-tool-choice --tool-call-parser hermes
    # command: --model Qwen/Qwen3-30B --port 80  --tensor-parallel-size 2 --quantization="fp8" --kv-cache-dtype fp8_e4m3  --enable-prefix-caching --reasoning-parser qwen3  --enable-auto-tool-choice --tool-call-parser hermes
    ports:
      - "0.0.0.0:8099:80"
    volumes:
      - ~/.hf-docker-data:/root/.cache/huggingface
    networks:
      - llmhost
    environment:
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]
    shm_size: '90g'
    ipc: host

  anythingllm:
    image: mintplexlabs/anythingllm
    ports:
    - "3001:3001"
    cap_add:
      - SYS_ADMIN
    environment:
    # Adjust for your environment
      - STORAGE_DIR=/app/server/storage
      - DISABLE_TELEMETRY=true
      - JWT_SECRET="a-complex-secret-key-1234567890abcdefGHIJKLmnopqrstuvwxyz!@#$%^&*()"
      - VECTOR_DB=lancedb
      - WHISPER_PROVIDER=local
      - TTS_PROVIDER=native
      - PASSWORDMINCHAR=8
      - LLM_PROVIDER=generic-openai
      # - GENERIC_OPEN_AI_BASE_PATH=http://text-generation-inference/v1
      - GENERIC_OPEN_AI_BASE_PATH=http://litellm:4000/v1
      - GENERIC_OPEN_AI_MODEL_PREF=default
      - GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT=128000
      - GENERIC_OPEN_AI_API_KEY=sk-1234
    volumes:
      - anythingllm_storage:/app/server/storage
    restart: unless-stopped
    networks:
      - llmhost

volumes:
  anythingllm_storage:
    # external: true
  pg_data:
    # external: true
  pgdata2:

networks:
  llmhost:
    driver: bridge
